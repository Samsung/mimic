%
%
\documentclass[]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{lmodern}
%\usepackage{textcomp}
%\usepackage[scaled=0.88]{beramono}
\usepackage{graphicx}

\usepackage{ucs}
\usepackage{longtable}

% At least 80% of every float page must be taken up by
% floats; there will be no page with more than 20% white space.
\def\topfraction{.95}
\def\dbltopfraction{\topfraction}
\def\floatpagefraction{\topfraction}     % default .5
\def\dblfloatpagefraction{\topfraction}  % default .5
\def\textfraction{.05}

\usepackage{amsthm}

\usepackage{listings}
\lstset{
    language=Java,
    basicstyle=\ttfamily\mdseries,
    identifierstyle=,
    stringstyle=\color{gray},
    numbers=left,
    numbersep=5pt,
    inputencoding=utf8x,
    xleftmargin=8pt,
    xrightmargin=8pt,
    keywordstyle=[1]\bfseries,
    keywordstyle=[2]\bfseries,
    keywordstyle=[3]\bfseries,
    keywordstyle=[4]\bfseries,
    numberstyle=\tiny,
    stepnumber=1,
    breaklines=true,
    frame=lines,
    showstringspaces=false,
    tabsize=2,
    commentstyle=\color{gray},
    captionpos=b
}
\newcommand{\code}[1]{\lstinline{#1}}

\newcommand{\bigO}[1]{\mathcal O\left(#1\right)} % big-o notation

% I find the text difficult to read with this enabled --MS
%\usepackage{fullpage}

\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}
\def\paragraphautorefname{Section}
\def\definitionautorefname{Definition}
\def\subsubsubsection{\paragraph}
\setcounter{secnumdepth}{4}


% macros for bnf grammers
\newenvironment{bnfgrammar}{\begin{center}\renewcommand{\arraystretch}{1.5}\begin{tabular}{rcl}}{\end{tabular}\end{center}}
\newcommand{\nonterminal}[1]{$\langle \textit{#1} \rangle$}
\newcommand{\production}[2]{#1 &$::=$& #2\\}
\newcommand{\literal}[1]{`$\text{\code{#1}}$'}
\newcommand{\alt}{$~\!\!\mid~$}
\newcommand{\altline}[1]{&$\mid$&#1\\}

\usepackage{mathtools}
\usepackage{amssymb,amsfonts,amsmath}
\usepackage[svgnames]{xcolor}
\usepackage[colorlinks=true,pdfborder={0 0 0},citecolor=DarkGreen,linkcolor=DarkBlue,urlcolor=DarkBlue]{hyperref}

\usepackage{amsthm}
% \usepackage{thmtools}
%\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\usepackage{framed}
\newenvironment{workinprogress}
{\color{gray} \begin{leftbar}}
{\end{leftbar}}

\newenvironment{new}{\begin{framed}}{\end{framed}}

\usepackage{booktabs}

\newcommand{\todo}[1]{{\textcolor{DarkRed}{ [\textbf{TODO}: #1]}}}

\parindent 0em
\setlength{\parskip}{.5em}

\title{Synthesis of Library Models via Dynamic Probing}
\author{
  Stefan Heule\\\url{sheule@cs.stanford.edu}
  \and
  Manu Sridharan\\\url{m.sridharan@samsung.com}
}

\begin{document}

\maketitle



\section{Motivation}

For any program analysis, static or dynamic, a key problem is handling of code
that, for one reason or another, cannot be analyzed.  Some code is either
unavailable or implemented in a difficult-to-analyze native language, e.g.,
implementations of the standard ECMAScript and DOM libraries in the browser.
Other code is simply difficult to analyze due to its size and complexity, e.g.,
frameworks like jQuery or even the Java standard library.  In realistic systems,
such code is often handled via hand-written models that expose its behavior to
analyses.  However, as the amount of such unanalyzable code grows, manual
writing of models becomes intractable. Furthermore, models must often be customized to
individual analyses in order to be useful, making it very difficult to re-use
models written for one analysis when a new analysis is developed.  A system to
automatically generate library models in a manner customized to the desired
client analysis could be of great use in making program analyses more easily
applicable to real systems.
\todo{clarify usefulness for scenario of performing
  dynamic analysis where code is available.  Could provide speedups?}

Specifically, automatic generation of library models could be useful in the following scenarios:
\begin{itemize}
\item Static pointer analyses for JavaScript such as WALA and TAJS
  \todo{citations?}  require models of built-in library routines in
  order to do a proper analysis of real-world programs, almost of
  which make use of these routines.  Both projects have invested
  significant efforts into manually writing such models, but still are
  missing models for many library routines, particularly browser DOM
  APIs.  With our proposed technique, at least some of these models
  could be automatically generated, saving significant effort.
\item Similarly, dynamic analyses also have a need for library models.
  For dynamic analysis, an unmodeled library method can simply be
  executed, and in principle its effects on its inputs could be
  observed and handled by the analysis.  However, in the presence of
  mutable heap data, directly observing effects at every library call
  would become far too complex and expensive.  Hence, manually-written
  models are often employed in practice, and automatic generation of
  these models would again be beneficial.
\end{itemize}

\section{Our approach}

We propose a technique for synthesizing library models via dynamic probing and
randomized search.  To learn the behaviors of a function without needing to
instrument its code, we repeatedly invoke the function with proxied objects as
arguments.  A proxied object allows for various low-level operations on the
object, such as field accesses, to be intercepted and customized.  Leveraging
this proxy functionality, we can dynamically probe the execution of a library
function and observe details of its behavior, even if the functionâ€™s code is
unavailable.  Proxy functionality is supported by a number of dynamic languages,
such as JavaScript, Python, and Ruby.

Executing a function with proxied arguments allows us to obtain
a trace of certain types of
operations it performed.
For instance, consider the following JavaScript function:
\begin{lstlisting}
  function copyField(o1, o2, field) {
    o1[field] = o2[field];
  }
\end{lstlisting}
If it is run with the inputs \code{\{\}}, \code{\{f: 10\}} and \code{"f"}, then its trace looks
like the following:
\begin{verbatim}
  read of field "f" of object o2, with resulting value 10
  write to field "f" of object o1 with value 10
\end{verbatim}
\todo{Sketch out any interesting details of our proxying algorithm, e.g., that we recursively
proxy objects}
Given this core trace generation
functionality, our model synthesis proceeds as follows.
We give the main, high-level description frist, and then talk about each
individual step in more detail.
We are given as inputs
to the synthesizer a function $f$ to model and an initial set of inputs.
First, we generate more inputs that we hope sufficiently covers the
space of behaviors of the function, using the initial inputs as a starting point.
We can then record a set of traces for each of these inputs on $f$,
learning the correct behavior of $f$ for those inputs.
From these traces, we try to infer any potential looping structure
the function might have, and then categorize the inputs so that similarly-behaving
inputs are grouped.
Next, we go through each of these input categories, and use a random search
to produce a program that performs as expected for this category
of inputs.
Finally, we combine the different programs from the different categories
by using conditional statements (and a random search to find the conditional
expression) and
clean up the resulting program with another random search.
This yields a model of the original function.

This is illustrated by the following pseudo-code:
\begin{verbatim}
procedure synthesize(f: Function, initialInputs: List) {
  var inputs = inputGeneration(f, initialInputs)
  var traces = inputs.map(i => recordTrace(f, i))
  var loop = inferLoop(traces)
  var categories = categorizeInputs(f, loop, traces)
  var subprograms = categories.map(category =>
    randomSearch(f, category.inputs, initialProgram(category))
  )
  var result = combinePrograms(subprograms)
  return improveProgram(result)
}
\end{verbatim}



\subsection{Trace Recording}

To record the behavior of a function, we rely on the proxy functionality
menioned earlier.  Instead of running the function for which we want to record
a trace directly on the inputs, we proxy all inputs first, and then run
the function on the proxied inputs instead.

It is sufficient to proxy the inputs ``shallowly'', instead of recursively
proxying all objects reachable from the list of arguments.  If the method
reads one of the fields of its inputs, the result of that field read can
be proxied lazily.

For every interaction with one of the proxies, an event is logged in the trace.
Finally, the result of the function is also logged, as well as whether it
threw an exception.


\subsection{Input Generation}

To generate a set of interesting inputs that are meant to explore different
behaviors of $f$, the following process is applied iteratively, starting with
the given set of inputs:
We use the current set of inputs to probe how the function uses these arguments,
and in particular which parts of the arguments get accessed by the function.
That is, we learn what access paths from which argument are followed, and
what the type of the various expressions are.
Based on this, new inputs are generated that differ in those parts of the
input.  For primitive inputs, we can generate new random inputs of the same
type, and there are some heuristics for arguments that are arrays or objects,
such as passing a longer or shorter array, or even an empty array.

This gives us a new set of inputs, and we can repeat the process (until a
fixpoint is reached, or we believe we have enough inputs).


\subsection{Loop Inference}

From the recorded set of traces, we try to infer the control flow graph
structure that our model should have.
Intuitively, a loop inside $f$ will result in a repeating pattern in the
events of the recorded trace.  Frequently, the values of the events (e.g.,
what value has been read, or which field is written to) change, but
the type of event (read, write, etc.) stay the same.  Thus, we
look at the \textit{skeleton} of a trace, i.e., the list of events with
the event argument erased (so that only the event type remains).

From just looking at the trace skeleton (but also the trace itself), it
is not possible to determine if a given proposed loop is correct;  this
can only be verified by trying to generate a program that contains this
loop and works for all inputs.
Therefore, we generate a set of proposed loops, and try one of them at
random (since the overal search strategy is repeated, all proposals
will be tried eventually).

To limit the number of proposals, we limit ourselfs to a single loop at
the moment, and generate all loops that could explain a given trace.  To
do this, the trace is inspected for repeating patterns of length 1, 2, etc.,
which corresponds to loop bodies of length 1, 2, etc.

This is repeated for all traces, giving us a large set of loop candidates.
To speed up the search, we rank them by checking each proposed loop against
all traces (and seeing if the loop can generate that trace), and secondly
favor programs with larger loop bodies.  When randomly picking a
loop, we give higher priority to higher ranking loops.

A possible alternative definition of a trace skeleton could include the
types of the event arguments (e.g., was a number written for a write event,
or a string?).  This would lead to less loop structure proposals, but might
miss certain loops.


\subsection{Input Categorization}

If the loop inference proposed a loop, then a first category consists of
all inputs that yield traces which can be explained by the proposed loop.
For the remaining inputs, the trace skeleton is used to categorize
the inputs.

\subsection{Initial Program}

For a category, the initial program is derived from the one of the traces
recorded.
Essentially, every event in the trace corresponds to a statement.  For instace,
a write event will give rise to a field assignment statement.

If the category comes with a loop proposal, then the repeating sequence
(the loop body) is replaced by the loop, and the loop body is derived from
one of the repeating event subsequences (e.g., the first one).


\subsection{Random Program Search}

The random search starts with an initial program, and is randomly mutated
and then evaluated.  If the program performs better according to the
metric, then becomes the new program.  Otherwise, with some probability,
we throw away the randomly changed program, or keep it even though it
has a lower score.

Pseudo-code outlining this search is given below:

\begin{verbatim}
procedure randomSearch(f, inputs, program) {
  var score = scoreModel(f, inputs, program)
  while (score > 0 && /* computational budget available */) {
    var newProgram = randomChange(program)
    var newScore = scoreModel(f, inputs, newProgram)
    
    // keep better programs
    if (newScore < score) {
      score = newScore
      program = newProgram
    } else {
      // with some probability, keep worse programs, too
      if (/* probability proportional to |newScore - score| */) {
        score = newScore
        program = newProgram
      }
    }
  }
  
  return program
}
\end{verbatim}



\subsection{Search Metric}

The hillclimbing random search needs a fitness function, or search metric, to
evaluate proposed random mutations.
This metric can be analysis dependent (different program anlyses might care
about different aspects), but is based on the recorded traces for all inputs.



\subsection{Combining Programs}

Once a program has been found for every category, these programs
are combined using conditional statements.  If the two programs
have a common prefix, then this prefix is moved in front of the conditional.
Finally, a random search is used to find the correct conditional
expression, starting with the constant \code{true}.

\subsection{Improving Result}

The last step in the search is to improve and clean up the program
that has been found, by removing unnecessary parts, or by making it
more readable.  This could involve a different search metric, that for instance
takes program length into account, or some analysis specific metric for what
parts are important.

\subsection{Handling of Non-Terminating Programs}

There are two sources of non-termination that are potentially problematic
in our approach.  First, the function $f$ could not terminate on some inputs.
Since we have no control over $f$, there is nothing we can do against that,
other than having a timeout and not consider inputs that cause $f$ to not terminate.

Secondly, we can generate a model that might not terminate.  For models that
include loops this is especially problematic, since it is easy for
a random program change to yield a non-terminating program.  However, we know
the behavior of the original function $f$ on that same input, and thus
have an idea of how long the execution of our model should be.  During
recording a trace for our model, if the sequence of events get's significantly
longer than the original trace, we can kill the execution and mark the
trace as having run out of it's computational budget.
By ranking such models poorly in the search metric, the random search will
avoid non-terminating programs.

\section*{Potential Concerns}

\begin{itemize}
  \item hard-to-execute functions which require a bunch of other setup code to
  run first
  \item handling DOM methods, where it's hard to undo changes.  In general,
  handling methods that have side effects on the network, file system, etc.
  \item what happens when we miss a case and model is insufficient for dynamic
  analysis.  Simple answer: we just ask the analysis writer to provide a model in
  those cases.  (Could also ask them to provide a template of a model.)  Or, we
  could take the input that is not handled and re-run the synthesis technique with
  that input added.
\end{itemize}











% \bibliographystyle{alpha}
% \bibliography{bib.bib}

\end{document}

%%% Local Variables:
%%% TeX-master: "synthesis"
%%% End:
